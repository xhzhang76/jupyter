{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 1 Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1.import library\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "### 2.import dataset\n",
    "dataset = pd.read_csv('Data.csv')\n",
    "X = dataset.iloc[ : , :-1].values   # iloc[row, col]\n",
    "Y = dataset.iloc[ : , 3].values     # : 全部行 or 列; [a]第a行 or 列; [a,b,c]第 a,b,c 行 or 列\n",
    "\n",
    "### 3.handle missing data\n",
    "from sklearn.preprocessing import Imputer\n",
    "imputer = Imputer(missing_values = \"NaN\", strategy = \"mean\", axis = 0)\n",
    "imputer = imputer.fit(X[:, 1:3])\n",
    "X[:, 1:3] = imputer.transform(X[:, 1:3])   # complete nan\n",
    "\n",
    "### 4.parse classification data\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "labelencoder_x = LabelEncoder()\n",
    "X[:, 0] = labelencoder_x.fit_transform(X[:, 0])\n",
    "#### create virtual variable\n",
    "onehotencoder = OneHotEncoder(categorical_features = [0])\n",
    "X = onehotencoder.fit_transform(X).toarray()\n",
    "labelencoder_y = LabelEncoder()\n",
    "Y = labelencoder_y.fit_transform(Y)\n",
    "\n",
    "### 5.split dataset into training and test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state = 0)\n",
    "\n",
    "### 6.feature scale\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc_X = StandardScaler()\n",
    "X_train = sc_X.fit_transform(X_train)\n",
    "X_test = sc_X.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 2 线性回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "### data pre-processing\n",
    "dataset = pd.read_csv('studentscores.csv')\n",
    "X = dataset.iloc[:, :1].values\n",
    "Y = dataset.iloc[:, 1].values\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 1/4, random_state = 0)\n",
    "\n",
    "### using linear regression train with train set\n",
    "from sklearn.linear_model import LinearRegression\n",
    "regressor = LinearRegression()\n",
    "regressor = regressor.fit(X_train, Y_train)\n",
    "\n",
    "### predict result\n",
    "Y_pred = regressor.predict(X_test)\n",
    "\n",
    "### visualization\n",
    "#### train set result visualization\n",
    "plt.scatter(X_train, Y_train, color = 'red')\n",
    "plt.plot(X_train, regressor.predict(X_train), color = 'blue')\n",
    "plt.show()\n",
    "#### test set result visualization\n",
    "plt.scatter(X_test, Y_test, color = 'red')\n",
    "plt.plot(X_test, regressor.predict(X_test), color = 'blue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 3 多元线性回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "### 数据预处理\n",
    "#### load dataset\n",
    "dataset = pd.read_csv('50_Startups.csv')\n",
    "X = dataset.iloc[:, :-1].values\n",
    "Y = dataset.iloc[:, 4].values\n",
    "#### 类别数据数字化\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "labelencoder = LabelEncoder()\n",
    "X[:, 3] = labelencoder.fit_transform(X[:, 3])\n",
    "#方法将类别与向量对应，例如{‘A’，‘B’，‘C’}分别与[1,0,0],[0,1,0],[0,0,1]对应，注意现在各个类别之间的欧式距离是相同的。\n",
    "onehotencoder = OneHotEncoder(categorical_features = [3])\n",
    "X = onehotencoder.fit_transform(X).toarray()\n",
    "####躲避虚拟变量陷阱，n-1法\n",
    "X = X[:, 1:]\n",
    "####拆分数据集为训练集和测试集\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=0)\n",
    "\n",
    "### 在训练集上训练多元线性回归模型\n",
    "from sklearn.linear_model import LinearRegression\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(X_train, Y_train)\n",
    "\n",
    "### 在测试集预测结果\n",
    "y_pred = regressor.predict(X_test)\n",
    "print(y_pred)\n",
    "print(Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day4 逻辑回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "###数据预处理\n",
    "dataset = pd.read_csv('Social_Network_Ads.csv')\n",
    "X = dataset.iloc[:, [2,3]].values\n",
    "Y = dataset.iloc[:, 4].values\n",
    "####将数据集分为训练集和测试集\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25, random_state=0)\n",
    "####特性缩放\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.fit_transform(X_test)\n",
    "\n",
    "###逻辑回归模型[]\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(X_train, Y_train)\n",
    "\n",
    "###预测测试集结果\n",
    "Y_pred = classifier.predict(X_test)\n",
    "\n",
    "###评估预测\n",
    "####生成混淆矩阵\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(Y_test, Y_pred)\n",
    "####可视化\n",
    "from matplotlib.colors import ListedColormap\n",
    "X_set, Y_set = X_train, Y_train\n",
    "X1, X2 = np.meshgrid(np.arange(start=X_set[:, 0].min()-1, stop=X_set[:,0].max()+1, step=0.01),\n",
    "                     np.arange(start=X_set[:, 1].min()-1, stop=X_set[:,1].max()+1, step=0.01))\n",
    "plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),\n",
    "             alpha=0.75, cmap=ListedColormap(('red', 'green')))\n",
    "plt.xlim(X1.min(), X1.max())\n",
    "plt.ylim(X2.min(), X2.max())\n",
    "for i,j in enumerate(np.unique(Y_set)):\n",
    "    plt.scatter(X_set[Y_set==j, 0], X_set[Y_set==j, 1],\n",
    "               c=ListedColormap(('red', 'green'))(i), label=j)\n",
    "plt.title('LOGISTIC(Training Set)')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Estimated Salary')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "X_set, Y_set = X_test, Y_test\n",
    "X1, X2 = np.meshgrid(np.arange(start=X_set[:, 0].min()-1, stop=X_set[:,0].max()+1, step=0.01),\n",
    "                     np.arange(start=X_set[:, 1].min()-1, stop=X_set[:,1].max()+1, step=0.01))\n",
    "plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),\n",
    "             alpha=0.75, cmap=ListedColormap(('red', 'green')))\n",
    "plt.xlim(X1.min(), X1.max())\n",
    "plt.ylim(X2.min(), X2.max())\n",
    "for i,j in enumerate(np.unique(Y_set)):\n",
    "    plt.scatter(X_set[Y_set==j, 0], X_set[Y_set==j, 1],\n",
    "               c=ListedColormap(('red', 'green'))(i), label=j)\n",
    "plt.title('LOGISTIC(Test Set)')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Estimated Salary')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
